{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контекстно-независимое векторное представление слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd() \n",
    "file_path = os.path.join(current_directory, 'data', 'news.txt.gz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, List\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "# Чтение файла данных\n",
    "def read_texts(fn: str=file_path) -> Iterator[Text]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield Text(*line.strip().split(\"\\t\"))\n",
    "                    \n",
    "# Разбиение текста на слова                 \n",
    "def tokenize_text(title: str, text: str) -> List[str]:\n",
    "    text = title.lower() + text.lower()\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ]', ' ', text)\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "    words = [t for t in words if t not in russian_stopwords and len(t) > 2]\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "# Текст без знаков припенания (нужен для gensim)\n",
    "def normalize_text(text: str) -> str:\n",
    "    return ' '.join(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('жанейр', 0.7212162613868713),\n",
       " ('марака', 0.619356095790863),\n",
       " ('итар', 0.5389225482940674),\n",
       " ('интерфакс', 0.5348697900772095),\n",
       " ('прайм', 0.5296918749809265),\n",
       " ('белт', 0.5146872401237488),\n",
       " ('агентств', 0.5137584209442139),\n",
       " ('собеседник', 0.5066440105438232),\n",
       " ('тасс', 0.506440281867981),\n",
       " ('корреспондент', 0.5062633752822876)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Обучение word2vec\n",
    "# каждый текст - набор слов через пробел\n",
    "texts = list(read_texts())\n",
    "sentences = [tokenize_text(text.title, text.text) for text in texts]\n",
    "labels = [text.label for text in texts]\n",
    "\n",
    "w2v = Word2Vec(sentences)\n",
    "\n",
    "w2v.wv.save_word2vec_format('w2v_vectors.bin')\n",
    "# пример\n",
    "w2v.wv.most_similar(\"новост\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усреднение векторов слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_mean(tokens, model):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "X_mean = []\n",
    "for tokens in sentences:\n",
    "    X_mean.append(vector_mean(tokens, w2v))\n",
    "\n",
    "X_mean = np.array(X_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.56      0.18      0.27        79\n",
      "     culture       0.86      0.84      0.85       279\n",
      "   economics       0.73      0.91      0.81       266\n",
      "      forces       0.72      0.83      0.77       149\n",
      "        life       0.77      0.78      0.78       288\n",
      "       media       0.80      0.77      0.79       299\n",
      "     science       0.85      0.82      0.83       288\n",
      "       sport       0.95      0.97      0.96       276\n",
      "       style       0.93      0.74      0.82        38\n",
      "      travel       0.63      0.45      0.52        38\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.78      0.73      0.74      2000\n",
      "weighted avg       0.81      0.81      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mean,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf усреднение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts_for_tfidf = [\" \".join(tokens) for tokens in sentences]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts_for_tfidf)\n",
    "vocab = tfidf.vocabulary_\n",
    "\n",
    "def vector_tfidf_mean(tokens, model, tfidf, vocab, doc_id):\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv and token in vocab:\n",
    "            token_id = vocab[token]\n",
    "            tfidf_weight = tfidf_matrix[doc_id, token_id]\n",
    "            vectors.append(model.wv[token] * tfidf_weight)\n",
    "            weights.append(tfidf_weight)\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.sum(vectors, axis=0) / np.sum(weights)\n",
    "\n",
    "X_tfidf_mean = []\n",
    "for i, tokens in enumerate(sentences):\n",
    "    X_tfidf_mean.append(vector_tfidf_mean(tokens, w2v, tfidf, vocab, i))\n",
    "\n",
    "X_tfidf_mean = np.array(X_tfidf_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.50      0.18      0.26        79\n",
      "     culture       0.86      0.84      0.85       279\n",
      "   economics       0.72      0.89      0.79       266\n",
      "      forces       0.73      0.81      0.77       149\n",
      "        life       0.75      0.77      0.76       288\n",
      "       media       0.80      0.74      0.77       299\n",
      "     science       0.82      0.83      0.82       288\n",
      "       sport       0.95      0.96      0.95       276\n",
      "       style       0.80      0.74      0.77        38\n",
      "      travel       0.67      0.47      0.55        38\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.76      0.72      0.73      2000\n",
      "weighted avg       0.79      0.80      0.79      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf_mean,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
